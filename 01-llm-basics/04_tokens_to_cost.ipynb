{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4e2bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "# Load .env file from parent directory\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "\n",
    "API_KEY = os.environ[\"GEMINI_API_KEY\"]\n",
    "\n",
    "client = genai.Client(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1599b23",
   "metadata": {},
   "source": [
    "## üîë Why Tokens Matter: The Economics\n",
    "\n",
    "Tokens are the CURRENCY of AI. Here's what you need to know:\n",
    "\n",
    "1. **You pay per token** (input + output)\n",
    "   - Every word you send costs money\n",
    "   - Every word the AI generates costs money\n",
    "\n",
    "2. **Different models have different prices**\n",
    "\n",
    "3. **One word ‚â† One token**\n",
    "   - \"Explain\" = 1 token\n",
    "   - \"database\" = 1 token... but uncommon words split into 2-3 tokens\n",
    "   - Punctuation, numbers, special characters all count\n",
    "\n",
    "4. **Context windows are limited**\n",
    "   - Every prompt you send uses up your available \"memory\"\n",
    "   - Long prompts = less space for conversation\n",
    "   - Less space = worse context awareness = worse answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e18896c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=11>\n",
      ") candidates=[Candidate(\n",
      "  content=Content(\n",
      "    parts=[\n",
      "      Part(\n",
      "        text=\"Hi Yash! I don't have a name. I am a large language model, an AI, trained by Google. It's nice to meet you!\"\n",
      "      ),\n",
      "    ],\n",
      "    role='model'\n",
      "  ),\n",
      "  finish_reason=<FinishReason.STOP: 'STOP'>,\n",
      "  index=0\n",
      ")] create_time=None model_version='gemini-2.5-flash' prompt_feedback=None response_id='vTqLaeX7H8vZjuMPucuDsQ4' usage_metadata=GenerateContentResponseUsageMetadata(\n",
      "  candidates_token_count=33,\n",
      "  prompt_token_count=11,\n",
      "  prompt_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=11\n",
      "    ),\n",
      "  ],\n",
      "  thoughts_token_count=203,\n",
      "  total_token_count=247\n",
      ") automatic_function_calling_history=[] parsed=None\n"
     ]
    }
   ],
   "source": [
    "# Example 1: SHORT PROMPT\n",
    "prompt_1 = \"Hi Name is Yash. What is your name?\"\n",
    "\n",
    "# Get response\n",
    "response_1 = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt_1\n",
    ")\n",
    "\n",
    "print(f\"Response: {response_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23471ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response text: Hi Yash! I don't have a name. I am a large language model, an AI, trained by Google. It's nice to meet you!\n",
      "\n",
      "üìä TOTAL TOKEN COUNT: 247\n",
      "   Input tokens (prompt): 11\n",
      "   Output tokens (response): 33\n",
      "   Thought tokens (Thinking): 203\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response text: {response_1.text}\")\n",
    "print(f\"\\nüìä TOTAL TOKEN COUNT: {response_1.usage_metadata.total_token_count}\")\n",
    "print(f\"   Input tokens (prompt): {response_1.usage_metadata.prompt_token_count}\")\n",
    "print(f\"   Output tokens (response): {response_1.usage_metadata.candidates_token_count}\")\n",
    "print(f\"   Thought tokens (Thinking): {response_1.usage_metadata.thoughts_token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2994c",
   "metadata": {},
   "source": [
    "## Example 2: BAD PROMPT ‚ùå\n",
    "Vague prompt leads to verbose, unfocused output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a59aa568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response text: A database is an **organized collection of information (data)** that is stored and managed electronically.\n",
      "\n",
      "Think of it like a highly structured digital filing cabinet. Instead of just a pile of documents, a database arranges data (like names, addresses, product details, transaction records) into tables, rows, and columns, making it easy to find, access, and update specific pieces of information quickly and efficiently.\n",
      "\n",
      "**In essence:** It's a system for storing, retrieving, manipulating, and managing data. You interact with databases constantly ‚Äì when you shop online, check your bank balance, use social media, or even book a flight.\n",
      "\n",
      "üìä TOTAL TOKEN COUNT: 934\n",
      "   Input tokens (prompt): 6\n",
      "   Output tokens (response): 128\n",
      "   Thought tokens (Thinking): 800\n"
     ]
    }
   ],
   "source": [
    "# Example 2: BAD PROMPT - Vague and lacks constraints\n",
    "prompt_2 = \"\"\"Explain database in brief.\"\"\"\n",
    "\n",
    "# Get response\n",
    "response_2 = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt_2\n",
    ")\n",
    "\n",
    "print(f\"Response text: {response_2.text}\")\n",
    "print(f\"\\nüìä TOTAL TOKEN COUNT: {response_2.usage_metadata.total_token_count}\")\n",
    "print(f\"   Input tokens (prompt): {response_2.usage_metadata.prompt_token_count}\")\n",
    "print(f\"   Output tokens (response): {response_2.usage_metadata.candidates_token_count}\")\n",
    "print(f\"   Thought tokens (Thinking): {response_2.usage_metadata.thoughts_token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d2debe",
   "metadata": {},
   "source": [
    "## Example 3: EFFICIENT PROMPT ‚úÖ\n",
    "Detailed instructions with clear constraints lead to focused, concise output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf3ba259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response text: Hey there! Think of a database as a super organized digital filing cabinet that stores information, like names, numbers, or grades, so it's easy to find and manage. For example, your college uses a giant database to keep track of every student's courses, grades, and contact info. Databases are crucial because they allow us to store vast amounts of data reliably and access specific pieces of information almost instantly. This speedy organization helps everything from your favorite online store to your university's registration system run smoothly and efficiently!\n",
      "\n",
      "üìä TOTAL TOKEN COUNT: 488\n",
      "   Input tokens (prompt): 84\n",
      "   Output tokens (response): 106\n",
      "   Thought tokens (Thinking): 298\n"
     ]
    }
   ],
   "source": [
    "# Example 3: EFFICIENT PROMPT - Clear instructions and constraints\n",
    "prompt_3 = \"\"\"You are a helpful assistant for college students.\n",
    "Your job is to explain technical concepts in simple language.\n",
    "Use examples whenever possible.\n",
    "Avoid jargon and technical terms.\n",
    "Be friendly and encouraging.\n",
    "Keep responses concise but complete.\n",
    "\n",
    "Now, explain what a database is to a college student.\n",
    "Include a real-world example.\n",
    "Explain why databases are important.\n",
    "Keep it under 5 sentences.\"\"\"\n",
    "\n",
    "# Get response\n",
    "response_3 = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt_3\n",
    ")\n",
    "\n",
    "print(f\"Response text: {response_3.text}\")\n",
    "print(f\"\\nüìä TOTAL TOKEN COUNT: {response_3.usage_metadata.total_token_count}\")\n",
    "print(f\"   Input tokens (prompt): {response_3.usage_metadata.prompt_token_count}\")\n",
    "print(f\"   Output tokens (response): {response_3.usage_metadata.candidates_token_count}\")\n",
    "print(f\"   Thought tokens (Thinking): {response_3.usage_metadata.thoughts_token_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09119d6c",
   "metadata": {},
   "source": [
    "## üìä Cost Analysis: Why Prompt Engineering Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe610bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üí∞ COST CALCULATION - Focusing on Prompt Engineering Impact\n",
    "# Based on Claude Pricing: Input $0.50/1M tokens, Output $3.00/1M tokens\n",
    "\n",
    "# Pricing (per token)\n",
    "input_price_per_1m = 0.50  # $0.50 per 1M input tokens\n",
    "output_price_per_1m = 3.00  # $3.00 per 1M output tokens\n",
    "\n",
    "input_price_per_token = input_price_per_1m / 1_000_000\n",
    "output_price_per_token = output_price_per_1m / 1_000_000\n",
    "\n",
    "# print(\"=\" * 70)\n",
    "# print(\"üí∞ COST BREAKDOWN FOR ALL 3 EXAMPLES\".center(70))\n",
    "# print(\"=\" * 70)\n",
    "# print(f\"\\nPricing: ${input_price_per_1m}/1M input tokens | ${output_price_per_1m}/1M output tokens\")\n",
    "# print(f\"         ${input_price_per_token:.2e}/token input | ${output_price_per_token:.2e}/token output\\n\")\n",
    "\n",
    "# Example 1 Cost\n",
    "example_1_input = response_1.usage_metadata.prompt_token_count\n",
    "example_1_output = response_1.usage_metadata.candidates_token_count + (response_1.usage_metadata.thoughts_token_count or 0)\n",
    "example_1_input_cost = example_1_input * input_price_per_token\n",
    "example_1_output_cost = example_1_output * output_price_per_token\n",
    "example_1_total_cost = example_1_input_cost + example_1_output_cost\n",
    "\n",
    "# print(\"üìå EXAMPLE 1: SIMPLE PROMPT (Baseline)\")\n",
    "# print(f\"   Input tokens: {example_1_input:,} ‚Üí ${example_1_input_cost:.6f}\")\n",
    "# print(f\"   Output tokens: {example_1_output:,} ‚Üí ${example_1_output_cost:.6f}\")\n",
    "# print(f\"   üíµ TOTAL COST: ${example_1_total_cost:.6f}\\n\")\n",
    "\n",
    "# Example 2 Cost (BAD PROMPT)\n",
    "example_2_input = response_2.usage_metadata.prompt_token_count\n",
    "example_2_output = response_2.usage_metadata.candidates_token_count + (response_2.usage_metadata.thoughts_token_count or 0)\n",
    "example_2_input_cost = example_2_input * input_price_per_token\n",
    "example_2_output_cost = example_2_output * output_price_per_token\n",
    "example_2_total_cost = example_2_input_cost + example_2_output_cost\n",
    "\n",
    "# print(\"üìå EXAMPLE 2: BAD PROMPT ‚ùå (Vague, no constraints)\")\n",
    "# print(f\"   Input tokens: {example_2_input:,} ‚Üí ${example_2_input_cost:.6f}\")\n",
    "# print(f\"   Output tokens: {example_2_output:,} ‚Üí ${example_2_output_cost:.6f}\")\n",
    "# print(f\"   üíµ TOTAL COST: ${example_2_total_cost:.6f}\")\n",
    "# print(f\"   ‚ö†Ô∏è  Less input but MORE output = Higher cost!\\n\")\n",
    "\n",
    "# Example 3 Cost (EFFICIENT PROMPT)\n",
    "example_3_input = response_3.usage_metadata.prompt_token_count\n",
    "example_3_output = response_3.usage_metadata.candidates_token_count + (response_3.usage_metadata.thoughts_token_count or 0)\n",
    "example_3_input_cost = example_3_input * input_price_per_token\n",
    "example_3_output_cost = example_3_output * output_price_per_token\n",
    "example_3_total_cost = example_3_input_cost + example_3_output_cost\n",
    "\n",
    "# print(\"üìå EXAMPLE 3: EFFICIENT PROMPT ‚úÖ (Clear instructions + constraints)\")\n",
    "# print(f\"   Input tokens: {example_3_input:,} ‚Üí ${example_3_input_cost:.6f}\")\n",
    "# print(f\"   Output tokens: {example_3_output:,} ‚Üí ${example_3_output_cost:.6f}\")\n",
    "# print(f\"   üíµ TOTAL COST: ${example_3_total_cost:.6f}\")\n",
    "# print(f\"   ‚ú® More input but CONTROLLED output = Better value!\\n\")\n",
    "\n",
    "# Detailed Comparison\n",
    "print(\"=\" * 60)\n",
    "print(\"üéØ KEY INSIGHTS: PROMPT ENGINEERING\".center(60))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüí° COMPARING EXAMPLE 2 (Bad) vs EXAMPLE 3 (Efficient):\\n\")\n",
    "print(f\"   Example 2 (Bad Prompt):\")\n",
    "print(f\"   ‚Ä¢ Input: {example_2_input} tokens (vague: \\\"Explain database in brief\\\")\")\n",
    "print(f\"   ‚Ä¢ Output: {example_2_output} tokens (verbose, unfocused response)\")\n",
    "print(f\"   ‚Ä¢ Cost: ${example_2_total_cost:.6f}\\n\")\n",
    "\n",
    "print(f\"   Example 3 (Efficient Prompt):\")\n",
    "print(f\"   ‚Ä¢ Input: {example_3_input} tokens (detailed instructions + constraints)\")\n",
    "print(f\"   ‚Ä¢ Output: {example_3_output} tokens (focused, concise response)\")\n",
    "print(f\"   ‚Ä¢ Cost: ${example_3_total_cost:.6f}\\n\")\n",
    "\n",
    "input_ratio = example_3_input / example_2_input\n",
    "output_diff_pct = ((example_2_output - example_3_output) / example_3_output) * 100\n",
    "cost_diff = example_3_total_cost - example_2_total_cost\n",
    "cost_diff_pct = (cost_diff / example_2_total_cost) * 100\n",
    "\n",
    "print(f\"üîç ANALYSIS:\")\n",
    "print(f\"   ‚Ä¢ Example 3 uses {input_ratio:.0f}x MORE input tokens ({example_3_input} vs {example_2_input})\")\n",
    "print(f\"   ‚Ä¢ But Example 2 generates {abs(output_diff_pct):.0f}% MORE output tokens ({example_2_output} vs {example_3_output})\")\n",
    "print(f\"   ‚Ä¢ Cost difference: Only ${abs(cost_diff):.6f} ({abs(cost_diff_pct):.1f}% more)\\n\")\n",
    "\n",
    "print(f\"‚úÖ THE LESSON:\")\n",
    "print(f\"   Bad prompts = Uncontrolled output = Wasted tokens = Higher costs\")\n",
    "print(f\"   Good prompts = Controlled output = Efficient tokens = Better value\\n\")\n",
    "print(f\"   Investing in input tokens (clear instructions) pays off by:\")\n",
    "print(f\"   1. Reducing unnecessary output tokens\")\n",
    "print(f\"   2. Getting exactly what you need\")\n",
    "print(f\"   3. Avoiding multiple retry attempts\")\n",
    "print(f\"   4. Saving money in the long run\\n\")\n",
    "\n",
    "total_all_costs = example_1_total_cost + example_2_total_cost + example_3_total_cost\n",
    "print(f\"üí∞ Grand Total (All 3 Examples): ${total_all_costs:.6f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
