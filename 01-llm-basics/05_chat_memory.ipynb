{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "## The Illusion of \"Memory\"\n",
    "\n",
    "Many of you will know this already. But for those that don't -- this might be an \"AHA\" moment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file from parent directory\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "\n",
    "API_KEY = os.environ[\"GEMINI_API_KEY\"]\n",
    "client = genai.Client(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "### Let's introduce ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "no_memory_1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Assistant: Hi Yash! It's great to meet you!\n",
      "\n",
      "I'm a large language model, designed to assist you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# First message: Introduce yourself\n",
    "response_1 = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Hi! I'm Yash!\"\n",
    ")\n",
    "\n",
    "print(\"ðŸ¤– Assistant:\", response_1.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8adca",
   "metadata": {},
   "source": [
    "### OK let's now ask a follow-up question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "no_memory_2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Assistant: As an AI, I don't know your name. I don't have access to personal information about you unless you choose to tell me.\n"
     ]
    }
   ],
   "source": [
    "# Second message: Ask about your name (WITHOUT sending previous context)\n",
    "response_2 = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"What's my name?\"\n",
    ")\n",
    "\n",
    "print(\"ðŸ¤– Assistant:\", response_2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f630f111",
   "metadata": {},
   "source": [
    "### Wait, what??\n",
    "\n",
    "We just told you!\n",
    "\n",
    "What's going on??\n",
    "\n",
    "Here's the thing: every call to an LLM is completely STATELESS. It's a totally new call, every single time. As AI engineers, it's OUR JOB to devise techniques to give the impression that the LLM has a \"memory\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "memory_demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Assistant: Your name is Yash! You told me that a moment ago.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Build conversation history using types.Content\n",
    "messages = [\n",
    "    types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=\"Hi! I'm Yash!\")]\n",
    "    ),\n",
    "    types.Content(\n",
    "        role=\"model\",\n",
    "        parts=[types.Part(text=\"Hi Yash! How can I assist you today?\")]\n",
    "    ),\n",
    "    types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=\"What's my name?\")]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Get response\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=messages\n",
    ")\n",
    "\n",
    "print(\"ðŸ¤– Assistant:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c36ec3e",
   "metadata": {},
   "source": [
    "### OpenAI Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82fad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm Yash!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi Yash! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21444d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.completions.create(model=\"gpt-5-nano\", messages=messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete_example",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Complete Example: Multi-Turn Conversation\n",
    "\n",
    "Let's have a longer conversation to really see how this works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Start fresh conversation\n",
    "messages = []\n",
    "\n",
    "def chat(user_message):\n",
    "    \"\"\"Helper function to send a message and get response\"\"\"\n",
    "    # Add user message using types.Content and types.Part\n",
    "    messages.append(\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part(text=user_message)]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Get response with full history\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=messages\n",
    "    )\n",
    "    \n",
    "    # Add assistant response to history using types.Content and types.Part\n",
    "    messages.append(\n",
    "        types.Content(\n",
    "            role=\"model\",\n",
    "            parts=[types.Part(text=response.text)]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "# Have a conversation\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ’¬ MULTI-TURN CONVERSATION\".center(70))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nðŸ‘¤ User: Hi! I'm Yash and I love coding in Python!\")\n",
    "response = chat(\"Hi! I'm Yash and I love coding in Python!\")\n",
    "print(f\"ðŸ¤– Assistant: {response}\")\n",
    "\n",
    "print(\"\\nðŸ‘¤ User: What's my name?\")\n",
    "response = chat(\"What's my name?\")\n",
    "print(f\"ðŸ¤– Assistant: {response}\")\n",
    "\n",
    "print(\"\\nðŸ‘¤ User: What programming language do I like?\")\n",
    "response = chat(\"What programming language do I like?\")\n",
    "print(f\"ðŸ¤– Assistant: {response}\")\n",
    "\n",
    "print(\"\\nðŸ‘¤ User: Can you write me a simple Python function?\")\n",
    "response = chat(\"Can you write me a simple Python function?\")\n",
    "print(f\"ðŸ¤– Assistant: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "view_history",
   "metadata": {},
   "source": [
    "## ðŸ“œ Let's View the Full Conversation History\n",
    "\n",
    "This is what we're actually sending to the API each time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show_history",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ðŸ“œ FULL CONVERSATION HISTORY\".center(70))\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTotal messages in history: {len(messages)}\\n\")\n",
    "\n",
    "for i, msg in enumerate(messages, 1):\n",
    "    # Access properties directly from types.Content object\n",
    "    role_emoji = \"ðŸ‘¤\" if msg.role == \"user\" else \"ðŸ¤–\"\n",
    "    role_name = \"User\" if msg.role == \"user\" else \"Assistant\"\n",
    "    \n",
    "    # Access text from types.Part object\n",
    "    content = msg.parts[0].text[:100] + \"...\" if len(msg.parts[0].text) > 100 else msg.parts[0].text\n",
    "    \n",
    "    print(f\"{i}. {role_emoji} {role_name}: {content}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key_takeaways",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **LLMs are stateless** - They don't remember anything between API calls\n",
    "\n",
    "2. **You create the illusion of memory** - By sending the full conversation history each time\n",
    "\n",
    "3. **Message structure matters** - Use proper role names (\"user\" and \"model\" for Gemini)\n",
    "\n",
    "4. **Costs grow with conversation length** - Each message includes ALL previous messages\n",
    "\n",
    "5. **Manage your context** - For long conversations, you might need to:\n",
    "   - Summarize old messages\n",
    "   - Remove irrelevant history\n",
    "   - Start fresh conversations\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Now you understand how chat memory works! In real applications, you'll need to:\n",
    "- Store conversation history in a database\n",
    "- Implement context window management\n",
    "- Handle conversation summarization\n",
    "- Optimize for cost and performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
